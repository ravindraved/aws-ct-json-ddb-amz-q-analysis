{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: CloudTrail S3 to EC2 Data Pipeline\n",
    "\n",
    "This notebook orchestrates the download, decompression, and validation of CloudTrail logs from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "src_path = Path('../src').resolve()\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "from common.config import Phase1Config, S3Config, AuthConfig\n",
    "from common.logging_config import setup_logging\n",
    "from phase1.s3_reader import S3CloudTrailReader\n",
    "from phase1.decompressor import FileDecompressor\n",
    "from phase1.validator import IntegrityValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = setup_logging(log_level=\"INFO\", log_file=\"../data/reports/phase1.log\")\n",
    "\n",
    "# Configuration\n",
    "config_dict = {\n",
    "    's3_config': {\n",
    "        'bucket_arn': 'arn:aws:s3:::aws-cloudtrail-logs-123456789112-72b9c5ab-ctrail-cwlog-bucket-name',\n",
    "        'prefix': 'AWSLogs/123456789112/CloudTrail/ap-south-1',\n",
    "        'start_date': '2025/07/25',\n",
    "        'end_date': '2025/07/27'  # Single day for testing\n",
    "    },\n",
    "    'auth_config': {\n",
    "        'method': 'instance_profile'  # Change to 'access_keys' if needed\n",
    "    },\n",
    "    'local_base_path': '../data'\n",
    "}\n",
    "\n",
    "config = Phase1Config.from_dict(config_dict)\n",
    "print(f\"Configuration loaded for bucket: {config.s3_config.bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: List and Download CloudTrail Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 reader\n",
    "s3_reader = S3CloudTrailReader(config.s3_config, config.auth_config)\n",
    "\n",
    "# List objects\n",
    "print(\"Listing CloudTrail objects...\")\n",
    "s3_objects = s3_reader.list_objects()\n",
    "print(f\"Found {len(s3_objects)} files to download\")\n",
    "\n",
    "# Display first few objects\n",
    "for obj in s3_objects[:3]:\n",
    "    print(f\"  {obj['key']} ({obj['size']} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "raw_path = Path(config.local_base_path) / 'raw'\n",
    "download_results = []\n",
    "\n",
    "print(\"Downloading files...\")\n",
    "for obj in tqdm(s3_objects, desc=\"Downloading\"):\n",
    "    local_file = raw_path / obj['key']\n",
    "    \n",
    "    success = s3_reader.download_file(obj['key'], local_file)\n",
    "    if success:\n",
    "        validated = s3_reader.validate_download(\n",
    "            obj['key'], local_file, obj['size'], obj['etag']\n",
    "        )\n",
    "        download_results.append({\n",
    "            'key': obj['key'],\n",
    "            'downloaded': success,\n",
    "            'validated': validated\n",
    "        })\n",
    "    else:\n",
    "        download_results.append({\n",
    "            'key': obj['key'],\n",
    "            'downloaded': False,\n",
    "            'validated': False\n",
    "        })\n",
    "\n",
    "successful_downloads = sum(1 for r in download_results if r['downloaded'])\n",
    "print(f\"Successfully downloaded {successful_downloads}/{len(s3_objects)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Decompress Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decompressor\n",
    "decompressor = FileDecompressor()\n",
    "processed_path = Path(config.local_base_path) / 'processed'\n",
    "\n",
    "decompression_results = []\n",
    "\n",
    "print(\"Decompressing files...\")\n",
    "for result in tqdm(download_results, desc=\"Decompressing\"):\n",
    "    if not result['downloaded']:\n",
    "        continue\n",
    "    \n",
    "    gz_file = raw_path / result['key']\n",
    "    json_file = processed_path / result['key'].replace('.gz', '.json')\n",
    "    \n",
    "    decomp_success = decompressor.decompress_file(gz_file, json_file)\n",
    "    if decomp_success:\n",
    "        json_valid = decompressor.validate_json(json_file)\n",
    "        decompression_results.append({\n",
    "            'key': result['key'],\n",
    "            'decompressed': decomp_success,\n",
    "            'json_valid': json_valid\n",
    "        })\n",
    "    else:\n",
    "        decompression_results.append({\n",
    "            'key': result['key'],\n",
    "            'decompressed': False,\n",
    "            'json_valid': False\n",
    "        })\n",
    "\n",
    "successful_decompressions = sum(1 for r in decompression_results if r['decompressed'])\n",
    "print(f\"Successfully decompressed {successful_decompressions}/{successful_downloads} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Integrity Validation and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize validator\n",
    "validator = IntegrityValidator(s3_objects, raw_path, processed_path)\n",
    "\n",
    "# Run validation\n",
    "print(\"Running integrity validation...\")\n",
    "validation_result = validator.validate()\n",
    "\n",
    "# Generate report\n",
    "report_path = Path(config.local_base_path) / 'reports' / 'integrity_report.json'\n",
    "validator.generate_report(validation_result, report_path)\n",
    "\n",
    "print(f\"\\nValidation Summary:\")\n",
    "print(f\"  Total S3 files: {validation_result.total_s3_files}\")\n",
    "print(f\"  Downloaded files: {validation_result.downloaded_files}\")\n",
    "print(f\"  Processed files: {validation_result.decompressed_files}\")\n",
    "print(f\"  Success rate: {validation_result.success_rate:.2f}%\")\n",
    "print(f\"\\nReport saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 1 Pipeline Complete!\")\n",
    "print(f\"\\nProcessed data location: {processed_path}\")\n",
    "print(f\"Ready for Phase 2 analysis with DuckDB\")\n",
    "\n",
    "# Show sample of processed data structure\n",
    "json_files = list(processed_path.rglob('*.json'))\n",
    "if json_files:\n",
    "    print(f\"\\nSample processed files:\")\n",
    "    for f in json_files[:3]:\n",
    "        print(f\"  {f.relative_to(processed_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
