{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuckDB Performance Analyzer for CloudTrail Data\n",
    "\n",
    "This notebook analyzes your processed CloudTrail data and provides technical insights about DuckDB query performance, memory requirements, and optimal query strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Add src to path\n",
    "src_path = Path('../src').resolve()\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from phase2.duckdb_connector import DuckDBConnector\n",
    "from common.logging_config import setup_logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Configuration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze system resources\n",
    "def analyze_system_config():\n",
    "    memory = psutil.virtual_memory()\n",
    "    disk = psutil.disk_usage('/')\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    \n",
    "    print(\"üñ•Ô∏è  SYSTEM CONFIGURATION ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üíæ Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"üíæ Available RAM: {memory.available / (1024**3):.1f} GB ({memory.percent:.1f}% used)\")\n",
    "    print(f\"üíΩ Total Disk: {disk.total / (1024**3):.1f} GB\")\n",
    "    print(f\"üíΩ Free Disk: {disk.free / (1024**3):.1f} GB ({(disk.used/disk.total)*100:.1f}% used)\")\n",
    "    print(f\"‚ö° CPU Cores: {cpu_count}\")\n",
    "    \n",
    "    return {\n",
    "        'total_ram_gb': memory.total / (1024**3),\n",
    "        'available_ram_gb': memory.available / (1024**3),\n",
    "        'cpu_cores': cpu_count,\n",
    "        'disk_free_gb': disk.free / (1024**3)\n",
    "    }\n",
    "\n",
    "system_config = analyze_system_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze processed data volume and structure\n",
    "def analyze_data_volume(data_path=\"../data/processed\"):\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(\"‚ùå No processed data found!\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nüìä DATA VOLUME ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # File analysis\n",
    "    json_files = list(data_path.rglob('*.json'))\n",
    "    total_files = len(json_files)\n",
    "    \n",
    "    if total_files == 0:\n",
    "        print(\"‚ùå No JSON files found!\")\n",
    "        return None\n",
    "    \n",
    "    # Size analysis\n",
    "    total_size = sum(f.stat().st_size for f in json_files)\n",
    "    avg_file_size = total_size / total_files\n",
    "    \n",
    "    print(f\"üìÅ Total Files: {total_files:,}\")\n",
    "    print(f\"üìè Total Size: {total_size / (1024**2):.1f} MB ({total_size / (1024**3):.3f} GB)\")\n",
    "    print(f\"üìè Average File Size: {avg_file_size / 1024:.1f} KB\")\n",
    "    \n",
    "    # Date range analysis\n",
    "    dates = set()\n",
    "    date_file_count = defaultdict(int)\n",
    "    date_size = defaultdict(int)\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        parts = file_path.parts\n",
    "        if len(parts) >= 4:\n",
    "            try:\n",
    "                year, month, day = parts[-4], parts[-3], parts[-2]\n",
    "                date_str = f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "                dates.add(date_str)\n",
    "                date_file_count[date_str] += 1\n",
    "                date_size[date_str] += file_path.stat().st_size\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    \n",
    "    dates = sorted(dates)\n",
    "    print(f\"üìÖ Date Range: {min(dates)} to {max(dates)} ({len(dates)} days)\")\n",
    "    print(f\"üìÖ Files per Day: {total_files / len(dates):.0f} average\")\n",
    "    print(f\"üìÖ Data per Day: {(total_size / len(dates)) / (1024**2):.1f} MB average\")\n",
    "    \n",
    "    return {\n",
    "        'total_files': total_files,\n",
    "        'total_size_mb': total_size / (1024**2),\n",
    "        'total_size_gb': total_size / (1024**3),\n",
    "        'avg_file_size_kb': avg_file_size / 1024,\n",
    "        'date_range': (min(dates), max(dates)),\n",
    "        'total_days': len(dates),\n",
    "        'files_per_day': total_files / len(dates),\n",
    "        'mb_per_day': (total_size / len(dates)) / (1024**2),\n",
    "        'date_file_count': dict(date_file_count),\n",
    "        'date_size': {k: v/(1024**2) for k, v in date_size.items()}\n",
    "    }\n",
    "\n",
    "data_stats = analyze_data_volume()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze JSON structure and estimate event counts\n",
    "def analyze_json_structure(data_path=\"../data/processed\", sample_files=5):\n",
    "    data_path = Path(data_path)\n",
    "    json_files = list(data_path.rglob('*.json'))[:sample_files]\n",
    "    \n",
    "    if not json_files:\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nüî¨ JSON STRUCTURE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_events = 0\n",
    "    total_records = 0\n",
    "    field_counts = defaultdict(int)\n",
    "    event_sources = defaultdict(int)\n",
    "    \n",
    "    for i, file_path in enumerate(json_files):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            if 'Records' in data:\n",
    "                records = data['Records']\n",
    "                total_records += len(records)\n",
    "                \n",
    "                for record in records[:10]:  # Sample first 10 records\n",
    "                    total_events += 1\n",
    "                    \n",
    "                    # Count fields\n",
    "                    for field in record.keys():\n",
    "                        field_counts[field] += 1\n",
    "                    \n",
    "                    # Count event sources\n",
    "                    if 'eventSource' in record:\n",
    "                        event_sources[record['eventSource']] += 1\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error reading {file_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if total_records == 0:\n",
    "        print(\"‚ùå No valid CloudTrail records found!\")\n",
    "        return None\n",
    "    \n",
    "    # Estimate total events\n",
    "    avg_records_per_file = total_records / len(json_files)\n",
    "    estimated_total_events = int(avg_records_per_file * data_stats['total_files'])\n",
    "    \n",
    "    print(f\"üìã Sample Files Analyzed: {len(json_files)}\")\n",
    "    print(f\"üìã Records per File: {avg_records_per_file:.0f} average\")\n",
    "    print(f\"üìã Estimated Total Events: {estimated_total_events:,}\")\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  Common Fields ({len(field_counts)} total):\")\n",
    "    for field, count in sorted(field_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  - {field}: {count}/{total_events} records\")\n",
    "    \n",
    "    print(f\"\\nüåê Top Event Sources:\")\n",
    "    for source, count in sorted(event_sources.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  - {source}: {count} events\")\n",
    "    \n",
    "    return {\n",
    "        'avg_records_per_file': avg_records_per_file,\n",
    "        'estimated_total_events': estimated_total_events,\n",
    "        'field_counts': dict(field_counts),\n",
    "        'event_sources': dict(event_sources)\n",
    "    }\n",
    "\n",
    "if data_stats:\n",
    "    json_stats = analyze_json_structure()\n",
    "else:\n",
    "    json_stats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark DuckDB performance with sample queries\n",
    "def benchmark_duckdb_performance(data_path=\"../data/processed\"):\n",
    "    if not data_stats or not json_stats:\n",
    "        print(\"‚ùå Cannot benchmark without data analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n‚ö° DUCKDB PERFORMANCE BENCHMARKS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create temporary DuckDB connection\n",
    "    db_path = \"../data/performance_test.duckdb\"\n",
    "    db = DuckDBConnector(db_path)\n",
    "    \n",
    "    try:\n",
    "        # Create view\n",
    "        print(\"üîß Creating CloudTrail view...\")\n",
    "        start_time = time.time()\n",
    "        success = db.create_cloudtrail_view(data_path)\n",
    "        view_creation_time = time.time() - start_time\n",
    "        \n",
    "        if not success:\n",
    "            print(\"‚ùå Failed to create view\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úÖ View created in {view_creation_time:.2f} seconds\")\n",
    "        \n",
    "        # Benchmark queries\n",
    "        benchmarks = []\n",
    "        \n",
    "        # Test 1: Simple count\n",
    "        print(\"\\nüß™ Test 1: Simple COUNT query\")\n",
    "        start_time = time.time()\n",
    "        result = db.execute_query(\"SELECT COUNT(*) as total FROM cloudtrail\")\n",
    "        query_time = time.time() - start_time\n",
    "        total_events = result.iloc[0]['total']\n",
    "        \n",
    "        print(f\"  üìä Total Events: {total_events:,}\")\n",
    "        print(f\"  ‚è±Ô∏è  Query Time: {query_time:.2f} seconds\")\n",
    "        print(f\"  üöÄ Events/Second: {total_events/query_time:,.0f}\")\n",
    "        \n",
    "        benchmarks.append({\n",
    "            'test': 'Simple COUNT',\n",
    "            'query_time': query_time,\n",
    "            'events_processed': total_events,\n",
    "            'events_per_second': total_events/query_time\n",
    "        })\n",
    "        \n",
    "        # Test 2: Aggregation by service\n",
    "        print(\"\\nüß™ Test 2: GROUP BY aggregation\")\n",
    "        start_time = time.time()\n",
    "        result = db.execute_query(\"\"\"\n",
    "            SELECT eventSource, COUNT(*) as count \n",
    "            FROM cloudtrail \n",
    "            GROUP BY eventSource \n",
    "            ORDER BY count DESC \n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        query_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  üìä Services Found: {len(result)}\")\n",
    "        print(f\"  ‚è±Ô∏è  Query Time: {query_time:.2f} seconds\")\n",
    "        print(f\"  üöÄ Events/Second: {total_events/query_time:,.0f}\")\n",
    "        \n",
    "        benchmarks.append({\n",
    "            'test': 'GROUP BY aggregation',\n",
    "            'query_time': query_time,\n",
    "            'events_processed': total_events,\n",
    "            'events_per_second': total_events/query_time\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'view_creation_time': view_creation_time,\n",
    "            'total_events': total_events,\n",
    "            'benchmarks': benchmarks\n",
    "        }\n",
    "        \n",
    "    finally:\n",
    "        db.close()\n",
    "        # Clean up test database\n",
    "        if Path(db_path).exists():\n",
    "            Path(db_path).unlink()\n",
    "\n",
    "if data_stats and json_stats:\n",
    "    perf_stats = benchmark_duckdb_performance()\n",
    "else:\n",
    "    perf_stats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "def generate_recommendations():\n",
    "    if not all([data_stats, json_stats, perf_stats, system_config]):\n",
    "        print(\"‚ùå Cannot generate recommendations without complete analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nüéØ PERFORMANCE RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_events = perf_stats['total_events']\n",
    "    total_size_gb = data_stats['total_size_gb']\n",
    "    available_ram_gb = system_config['available_ram_gb']\n",
    "    total_days = data_stats['total_days']\n",
    "    \n",
    "    # Data size assessment\n",
    "    if total_size_gb < 0.1:\n",
    "        size_category = \"SMALL\"\n",
    "        size_emoji = \"üü¢\"\n",
    "    elif total_size_gb < 1.0:\n",
    "        size_category = \"MEDIUM\"\n",
    "        size_emoji = \"üü°\"\n",
    "    else:\n",
    "        size_category = \"LARGE\"\n",
    "        size_emoji = \"üî¥\"\n",
    "    \n",
    "    print(f\"\\n{size_emoji} DATA SIZE CATEGORY: {size_category}\")\n",
    "    print(f\"  üìä Total Data: {total_size_gb:.3f} GB ({total_events:,} events)\")\n",
    "    print(f\"  üíæ Available RAM: {available_ram_gb:.1f} GB\")\n",
    "    print(f\"  üìÖ Date Range: {total_days} days\")\n",
    "    \n",
    "    # Performance tier\n",
    "    avg_events_per_sec = sum(b['events_per_second'] for b in perf_stats['benchmarks']) / len(perf_stats['benchmarks'])\n",
    "    \n",
    "    if avg_events_per_sec > 100000:\n",
    "        perf_tier = \"EXCELLENT\"\n",
    "        perf_emoji = \"üöÄ\"\n",
    "    elif avg_events_per_sec > 50000:\n",
    "        perf_tier = \"GOOD\"\n",
    "        perf_emoji = \"‚ö°\"\n",
    "    else:\n",
    "        perf_tier = \"MODERATE\"\n",
    "        perf_emoji = \"üêå\"\n",
    "    \n",
    "    print(f\"\\n{perf_emoji} PERFORMANCE TIER: {perf_tier}\")\n",
    "    print(f\"  üèÉ Average Processing Speed: {avg_events_per_sec:,.0f} events/second\")\n",
    "    \n",
    "    # Optimal date range recommendations\n",
    "    print(f\"\\nüìÖ OPTIMAL DATE RANGE RECOMMENDATIONS:\")\n",
    "    \n",
    "    # Calculate safe date ranges based on memory\n",
    "    safe_days_for_select_all = int((available_ram_gb * 0.8) / (data_stats['mb_per_day'] / 1024 * 1.2))  # 80% RAM, 120% overhead\n",
    "    safe_days_for_aggregation = min(total_days, int((available_ram_gb * 0.9) / (data_stats['mb_per_day'] / 1024 * 0.1)))  # 90% RAM, 10% overhead\n",
    "    \n",
    "    print(f\"  üîç For SELECT * queries: {max(1, safe_days_for_select_all)} days maximum\")\n",
    "    print(f\"  üìä For aggregation queries: {safe_days_for_aggregation} days (full dataset OK)\")\n",
    "    print(f\"  ‚ö° For filtered queries: {min(total_days, 30)} days recommended\")\n",
    "    \n",
    "    # Query optimization tips\n",
    "    print(f\"\\nüí° QUERY OPTIMIZATION TIPS:\")\n",
    "    \n",
    "    tips = [\n",
    "        \"‚úÖ Use COUNT(*), GROUP BY, and aggregations - very memory efficient\",\n",
    "        \"‚úÖ Add WHERE clauses to filter data early (pushdown optimization)\",\n",
    "        \"‚úÖ Select specific columns instead of SELECT *\",\n",
    "        \"‚úÖ Use date filters: WHERE eventTime >= '2025-07-30'\",\n",
    "        \"‚ö†Ô∏è  Avoid SELECT * on large date ranges\",\n",
    "        \"‚ö†Ô∏è  Complex JOINs may require more memory\",\n",
    "        \"‚ùå Avoid ORDER BY without LIMIT on large results\"\n",
    "    ]\n",
    "    \n",
    "    for tip in tips:\n",
    "        print(f\"  {tip}\")\n",
    "    \n",
    "    return {\n",
    "        'size_category': size_category,\n",
    "        'performance_tier': perf_tier,\n",
    "        'safe_days_select_all': safe_days_for_select_all,\n",
    "        'safe_days_aggregation': safe_days_for_aggregation,\n",
    "        'avg_events_per_sec': avg_events_per_sec\n",
    "    }\n",
    "\n",
    "if data_stats and json_stats and perf_stats:\n",
    "    recommendations = generate_recommendations()\n",
    "else:\n",
    "    recommendations = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "def generate_summary_report():\n",
    "    if not all([data_stats, json_stats, perf_stats, recommendations]):\n",
    "        print(\"‚ùå Cannot generate summary without complete analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã DUCKDB PERFORMANCE ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key metrics\n",
    "    print(f\"\\nüéØ KEY METRICS:\")\n",
    "    print(f\"  üìä Total Events: {perf_stats['total_events']:,}\")\n",
    "    print(f\"  üìÅ Total Files: {data_stats['total_files']:,}\")\n",
    "    print(f\"  üìè Total Size: {data_stats['total_size_gb']:.3f} GB\")\n",
    "    print(f\"  üìÖ Date Range: {data_stats['date_range'][0]} to {data_stats['date_range'][1]}\")\n",
    "    print(f\"  üíæ Available RAM: {system_config['available_ram_gb']:.1f} GB\")\n",
    "    print(f\"  ‚ö° Processing Speed: {recommendations['avg_events_per_sec']:,.0f} events/sec\")\n",
    "    \n",
    "    # Status indicators\n",
    "    print(f\"\\nüö¶ STATUS INDICATORS:\")\n",
    "    print(f\"  üìä Data Size: {recommendations['size_category']}\")\n",
    "    print(f\"  ‚ö° Performance: {recommendations['performance_tier']}\")\n",
    "    print(f\"  üíæ Memory Fit: {'‚úÖ EXCELLENT' if data_stats['total_size_gb'] < system_config['available_ram_gb'] * 0.1 else 'üü° GOOD' if data_stats['total_size_gb'] < system_config['available_ram_gb'] * 0.5 else 'üî¥ TIGHT'}\")\n",
    "    \n",
    "    # Quick recommendations\n",
    "    print(f\"\\n‚ö° QUICK RECOMMENDATIONS:\")\n",
    "    print(f\"  üîç Safe for SELECT *: {recommendations['safe_days_select_all']} days\")\n",
    "    print(f\"  üìä Safe for aggregations: Full dataset ({data_stats['total_days']} days)\")\n",
    "    print(f\"  üéØ Optimal query range: 7-30 days\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ ANALYSIS COMPLETE - Your CloudTrail data is ready for efficient DuckDB querying!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if recommendations:\n",
    "    generate_summary_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
